{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration: TruthSeeker Dataset\n",
    "\n",
    "This notebook provides initial exploration of the TruthSeeker dataset components for analyzing misinformation spread by bots vs humans.\n",
    "\n",
    "## Objectives\n",
    "1. Load and inspect dataset components (FakeNewsNet, CoAID, TwiBot-22)\n",
    "2. Assess data quality and completeness\n",
    "3. Understand data structure and relationships\n",
    "4. Identify integration opportunities and challenges\n",
    "5. Generate preliminary statistics and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Define paths\n",
    "DATA_RAW = Path('../data/raw')\n",
    "DATA_PROCESSED = Path('../data/processed')\n",
    "DATA_EXTERNAL = Path('../data/external')\n",
    "RESULTS_FIGURES = Path('../results/figures')\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. FakeNewsNet Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FakeNewsNet data\n",
    "# NOTE: Update paths based on actual data structure after download\n",
    "\n",
    "fakenewsnet_path = DATA_EXTERNAL / 'FakeNewsNet'\n",
    "\n",
    "if fakenewsnet_path.exists():\n",
    "    print(f\"FakeNewsNet directory found at: {fakenewsnet_path}\")\n",
    "    \n",
    "    # List subdirectories\n",
    "    subdirs = [d for d in fakenewsnet_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Subdirectories: {[d.name for d in subdirs]}\")\n",
    "    \n",
    "    # TODO: Load actual data files once downloaded\n",
    "    # Example:\n",
    "    # politifact_news = pd.read_csv(fakenewsnet_path / 'politifact' / 'news.csv')\n",
    "    # gossipcop_news = pd.read_csv(fakenewsnet_path / 'gossipcop' / 'news.csv')\n",
    "else:\n",
    "    print(\"FakeNewsNet data not found. Please download using instructions in docs/data_acquisition.md\")\n",
    "    print(f\"Expected path: {fakenewsnet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FakeNewsNet Data Structure\n",
    "\n",
    "Expected components:\n",
    "- News articles (fake/real labels)\n",
    "- Social media posts (tweets sharing articles)\n",
    "- User engagement (retweets, likes, replies)\n",
    "- Temporal information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for FakeNewsNet analysis\n",
    "# Once data is loaded:\n",
    "# - Display basic statistics (number of articles, posts, users)\n",
    "# - Show label distribution (fake vs real)\n",
    "# - Temporal coverage\n",
    "# - Sample records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CoAID Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CoAID data\n",
    "coaid_path = DATA_EXTERNAL / 'CoAID'\n",
    "\n",
    "if coaid_path.exists():\n",
    "    print(f\"CoAID directory found at: {coaid_path}\")\n",
    "    \n",
    "    # List subdirectories\n",
    "    subdirs = [d for d in coaid_path.iterdir() if d.is_dir()]\n",
    "    print(f\"Subdirectories: {[d.name for d in subdirs]}\")\n",
    "    \n",
    "    # TODO: Load actual data files\n",
    "else:\n",
    "    print(\"CoAID data not found. Please download using instructions in docs/data_acquisition.md\")\n",
    "    print(f\"Expected path: {coaid_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CoAID Data Structure\n",
    "\n",
    "Expected components:\n",
    "- COVID-19 related news and claims\n",
    "- Social media posts\n",
    "- Fact-checking labels\n",
    "- Multi-modal content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for CoAID analysis\n",
    "# Once data is loaded:\n",
    "# - Dataset size and structure\n",
    "# - Label distribution\n",
    "# - Temporal coverage\n",
    "# - Sample records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TwiBot-22 Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TwiBot-22 data\n",
    "twibot_path = DATA_EXTERNAL / 'TwiBot-22'\n",
    "\n",
    "if twibot_path.exists():\n",
    "    print(f\"TwiBot-22 directory found at: {twibot_path}\")\n",
    "    \n",
    "    # List subdirectories and files\n",
    "    items = list(twibot_path.iterdir())\n",
    "    print(f\"Contents: {[item.name for item in items[:10]]}...\")  # First 10 items\n",
    "    \n",
    "    # TODO: Load actual data files\n",
    "    # Example:\n",
    "    # user_labels = pd.read_csv(twibot_path / 'labels.csv')\n",
    "    # user_profiles = pd.read_json(twibot_path / 'user.json', lines=True)\n",
    "else:\n",
    "    print(\"TwiBot-22 data not found. Please download using instructions in docs/data_acquisition.md\")\n",
    "    print(f\"Expected path: {twibot_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TwiBot-22 Data Structure\n",
    "\n",
    "Expected components:\n",
    "- User profiles (bots and humans)\n",
    "- Bot/human labels\n",
    "- Tweet content\n",
    "- Network relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for TwiBot-22 analysis\n",
    "# Once data is loaded:\n",
    "# - Number of bot vs human accounts\n",
    "# - Label distribution and confidence\n",
    "# - Account characteristics\n",
    "# - Sample records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Integration Planning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify common fields for integration\n",
    "# - User IDs (Twitter user IDs)\n",
    "# - Tweet IDs\n",
    "# - Timestamps\n",
    "\n",
    "# Integration strategy:\n",
    "# 1. Extract user IDs from FakeNewsNet/CoAID posts\n",
    "# 2. Match with TwiBot-22 user labels\n",
    "# 3. Create unified dataset with bot/human labels\n",
    "# 4. Preserve temporal and network information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template for data quality checks\n",
    "def assess_data_quality(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Assess data quality for a given dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas DataFrame\n",
    "        dataset_name: str, name of dataset for reporting\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Data Quality Assessment: {dataset_name}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\\n\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df) * 100).round(2)\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing,\n",
    "        'Percentage': missing_pct\n",
    "    }).sort_values('Percentage', ascending=False)\n",
    "    print(\"Missing Values:\")\n",
    "    print(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    # Duplicates\n",
    "    duplicates = df.duplicated().sum()\n",
    "    print(f\"\\nDuplicate rows: {duplicates} ({duplicates/len(df)*100:.2f}%)\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nData types:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# Will be applied to each dataset once loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Preliminary Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholder for visualizations\n",
    "# Once data is loaded, create:\n",
    "# 1. Distribution of fake vs real news\n",
    "# 2. Bot vs human label distribution\n",
    "# 3. Temporal distribution of posts\n",
    "# 4. User activity distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of exploration findings\n",
    "summary = {\n",
    "    'datasets_available': [],\n",
    "    'total_records': 0,\n",
    "    'integration_feasibility': 'TBD',\n",
    "    'data_quality_issues': [],\n",
    "    'next_steps': [\n",
    "        'Download all dataset components',\n",
    "        'Complete data quality assessment',\n",
    "        'Develop data integration pipeline',\n",
    "        'Create analysis-ready dataset',\n",
    "        'Proceed to RQ-specific analyses'\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPLORATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for key, value in summary.items():\n",
    "    print(f\"\\n{key.replace('_', ' ').title()}:\")\n",
    "    if isinstance(value, list):\n",
    "        for item in value:\n",
    "            print(f\"  - {item}\")\n",
    "    else:\n",
    "        print(f\"  {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
